# theria
theria is a research toolkit for higher-order learning with modern neural operators. It studies how meta-learning and implicit differentiation interact with kernel-level performance, starting from attention and extending to operator-based models.

## Project status

- Phase 0: Operator contract and reference semantics ✅
- Phase 1: First-order autograd correctness (CPU) ✅
- Phase 2: HVP/JVP + meta-learning support (in progress)
- Phase 3: Triton/CUDA kernels (future)

All current tests run on CPU.