wrapper,attention_backend,status,second_order_path_all,second_order_path_any,second_order_path_attn_all,second_order_path_attn_any,second_order_path_head_all,second_order_path_head_any,attn_second_order_ok,rel_diff_mean,error
baseline,custom,OK,True,True,True,True,True,True,True,0.573209,
detach_attention_output,custom,OK,True,True,False,False,True,True,False,0.246468,
no_grad_attention,custom,HARD_FAIL_UNUSED,,,,,,,False,,One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.
once_differentiable_sim,custom,HARD_FAIL_ONCE_DIFF,,,,,,,False,,trying to differentiate twice a function that was marked with @once_differentiable
stats_detach_logits_sdpa,custom,OK,True,True,True,True,True,True,False,0.362581,
stats_detach_softmax_output_sdpa,custom,OK,True,True,True,True,True,True,False,0.362581,
